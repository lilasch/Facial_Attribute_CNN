{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lilasch/Facial_Attribute_Classification/blob/main/liladatapreprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gUz8sUaAIqQj"
      },
      "source": [
        "**Facial Attribute Classification**\n",
        "\n",
        "Lila Schisgal and Phoebe Jeske\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EEiHEhSWtsT0"
      },
      "source": [
        "First, we import our data from two text files and partition it into training and validation data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "import zipfile\n",
        "zippath = 'img_align_celeba.zip'\n",
        "targetfolder = 'images'\n",
        "\n",
        "with zipfile.ZipFile(zippath, 'r') as zip_ref:\n",
        "    zip_ref.extractall(targetfolder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y-AnqGT9HudD",
        "outputId": "b8ff0655-f763-49e3-90ea-5b0ad9e84101"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/zz/p0bxtrc57nx_bmrp_9vns_r40000gn/T/ipykernel_1593/4022894428.py:17: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
            "  attributes = pd.read_csv(\"list_attr_celeba.txt\", skiprows = 1, delimiter=\"\\s+|\\t\")\n",
            "/var/folders/zz/p0bxtrc57nx_bmrp_9vns_r40000gn/T/ipykernel_1593/4022894428.py:21: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
            "  partitions = pd.read_csv(\"list_eval_partition.txt\", delimiter=\"\\s+|\\t\", header = None)\n"
          ]
        }
      ],
      "source": [
        "from tensorflow import keras\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import glob\n",
        "import shutil\n",
        "import os\n",
        "\n",
        "\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive', force_remount = True)\n",
        "\n",
        "# 40 attributes in total\n",
        "num_attributes = 40\n",
        "\n",
        "# Load attributes csv\n",
        "# attributes = pd.read_csv(\"/content/drive/MyDrive/list_attr_celeba.txt\", skiprows = 1, delimiter=\"\\s+|\\t\")\n",
        "attributes = pd.read_csv(\"list_attr_celeba.txt\", skiprows = 1, delimiter=\"\\s+|\\t\")\n",
        "\n",
        "# Load csv with partitions values\n",
        "# partitions = pd.read_csv(\"/content/drive/MyDrive/list_eval_partition.txt\", delimiter=\"\\s+|\\t\", header = None)\n",
        "partitions = pd.read_csv(\"list_eval_partition.txt\", delimiter=\"\\s+|\\t\", header = None)\n",
        "\n",
        "\n",
        "partitions.columns = ['image_name', 'dataset'] # setting column header names for partitions\n",
        "attributes['dataset'] = partitions['dataset'].values # copying the partition values into the attributes df\n",
        "\n",
        "# filter so just the eyeglasses attribute is included\n",
        "glasses_df = attributes.iloc[:, [15, num_attributes]]\n",
        "\n",
        "# partitioning the data\n",
        "train_df = glasses_df[glasses_df['dataset'] == 0]\n",
        "valid_df = glasses_df[glasses_df['dataset'] == 1]\n",
        "test_df = glasses_df[glasses_df['dataset'] == 2]\n",
        "\n",
        "y_train = train_df['Eyeglasses']\n",
        "y_valid = valid_df['Eyeglasses']\n",
        "y_test = test_df['Eyeglasses']\n",
        "\n",
        "# getting our image file names in two arrays\n",
        "train_files = train_df.index\n",
        "valid_files = valid_df.index\n",
        "test_files = test_df.index\n",
        "\n",
        "image_folder = \"images/img_align_celeba/\"\n",
        "valid_destination = \"valid_images\"\n",
        "train_destination = \"train_images\"\n",
        "test_destination = \"test_images\"\n",
        "\n",
        "# Function to split images based on classification\n",
        "def split_images(source_folder):\n",
        "    files = os.listdir(source_folder)  # Get the list of files in the source_folder\n",
        "    for file in files:\n",
        "        if file in train_files:\n",
        "            if not os.path.exists(os.path.join(train_destination, file)):\n",
        "                shutil.move(os.path.join(source_folder, file), os.path.join(train_destination, file))\n",
        "        elif file in valid_files:\n",
        "            if not os.path.exists(os.path.join(valid_destination, file)):\n",
        "                shutil.move(os.path.join(source_folder, file), os.path.join(valid_destination, file))\n",
        "        elif file in test_files:\n",
        "            if not os.path.exists(os.path.join(test_destination, file)):\n",
        "                shutil.move(os.path.join(source_folder, file), os.path.join(test_destination, file))\n",
        "        else:\n",
        "            continue\n",
        "\n",
        "split_images(image_folder)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qx3tdo-ptvGs"
      },
      "source": [
        "We then separate our target y values and image filenames in preparation for accessing the actual images as vectors."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cARSstsgtz11"
      },
      "source": [
        "Next: need to access the images from the dataset and put them into a set of vectors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HD7futHFwkgc",
        "outputId": "f9e8c7be-6870-49a9-da44-155f0cd7e0ae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "block started\n",
            "imported tensorflow\n",
            "imported keras\n",
            "imported pandas\n",
            "Images normalized\n",
            "valid_done\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Set your folder paths\n",
        "valid_folder = \"valid_images/\"\n",
        "test_folder = \"test_images/\"\n",
        "train_folder = \"train_images/\"\n",
        "\n",
        "# Function to normalize images\n",
        "def normalize_images(image_path):\n",
        "    save_array = []\n",
        "    for filename in os.listdir(image_path):\n",
        "        if filename.endswith(\".jpg\"):\n",
        "            img_path = os.path.join(image_path, filename)\n",
        "            img = Image.open(img_path)\n",
        "            img_array = np.array(img)\n",
        "            flattened_dim = 218 * 178 * 3\n",
        "            img_processed = img_array.reshape(-1, flattened_dim)\n",
        "            img_processed = img_processed.astype(float)\n",
        "            save_array.append(img_processed)\n",
        "    return save_array\n",
        "\n",
        "def standardize(train, test, valid):\n",
        "    mean = np.mean(train)\n",
        "    std = np.std(train)\n",
        "\n",
        "    train = (train - mean)/std\n",
        "    valid = (valid - mean)/std\n",
        "    test = (test - mean)/std\n",
        "\n",
        "    return train, test, valid\n",
        "\n",
        "\n",
        "\n",
        "# Apply preprocessing steps\n",
        "valid_images = normalize_images(valid_folder)\n",
        "train_images = normalize_images(train_folder)\n",
        "test_images = normalize_images(test_folder)\n",
        "\n",
        "x_train, x_test, x_valid = standardize(train_images, test_images, valid_images)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "block started\n",
            "tensorflow imported\n",
            "keras imported\n",
            "numpy imported\n",
            "matplotlib imported\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'x_train' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[3], line 26\u001b[0m\n\u001b[1;32m     16\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     17\u001b[0m               loss\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlosses\u001b[38;5;241m.\u001b[39mSparseCategoricalCrossentropy(from_logits\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[1;32m     18\u001b[0m               metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     20\u001b[0m early_stopping \u001b[38;5;241m=\u001b[39m keras\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mEarlyStopping(\n\u001b[1;32m     21\u001b[0m     patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m,\n\u001b[1;32m     22\u001b[0m     min_delta\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m,\n\u001b[1;32m     23\u001b[0m     restore_best_weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     24\u001b[0m )\n\u001b[0;32m---> 26\u001b[0m network_history_std \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(\u001b[43mx_train\u001b[49m, y_train, validation_data\u001b[38;5;241m=\u001b[39m(x_valid,y_valid),\n\u001b[1;32m     27\u001b[0m                                 epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000000000\u001b[39m, callbacks\u001b[38;5;241m=\u001b[39m[early_stopping])\n",
            "\u001b[0;31mNameError\u001b[0m: name 'x_train' is not defined"
          ]
        }
      ],
      "source": [
        "model = keras.Sequential()\n",
        "model.add(keras.layers.Dense(128, activation='relu', kernel_initializer='he_normal', kernel_regularizer=keras.regularizers.L2(0.01)))\n",
        "model.add(keras.layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "early_stopping = keras.callbacks.EarlyStopping(\n",
        "    patience=5,\n",
        "    min_delta=0.001,\n",
        "    restore_best_weights=True,\n",
        ")\n",
        "\n",
        "network_history_std = model.fit(x_train, y_train, validation_data=(x_valid,y_valid),\n",
        "                                epochs=1000000000, callbacks=[early_stopping])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "eval = model.evaluate(x_test,y_test)\n",
        "print(\"Test loss:\",eval[0])\n",
        "print(\"Test accuracy:\",eval[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
