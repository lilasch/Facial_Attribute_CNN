{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPLX+gHvcRnGe5w6A+vJ01F",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lilasch/Facial_Attribute_CNN/blob/main/Final_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Data Collection and Training"
      ],
      "metadata": {
        "id": "JrHKHS6qxArV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KIDfpwbtwQ6B"
      },
      "outputs": [],
      "source": [
        "import tensorflow\n",
        "import tensorflow.keras as keras\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import sklearn\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.layers import Dense, Dropout, Input, GlobalAveragePooling2D, BatchNormalization\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers.experimental.preprocessing import Resizing\n",
        "from tensorflow.keras import initializers\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from keras.preprocessing import image\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import classification_report, multilabel_confusion_matrix\n",
        "\n",
        "#First, we import our data from two text files and partition it into training and validation data.\n",
        "\n",
        "# Load attributes csv\n",
        "attributes = pd.read_csv(\"list_attr_celeba.txt\", skiprows = 1, delimiter=\"\\s+|\\t\")\n",
        "\n",
        "# Load csv with partitions values\n",
        "partitions = pd.read_csv(\"list_eval_partition.txt\", delimiter=\"\\s+|\\t\", header = None)\n",
        "\n",
        "partitions.columns = ['image_name', 'dataset'] # setting column header names for partitions\n",
        "attributes['dataset'] = partitions['dataset'].values # copying the partition values into the attributes df\n",
        "\n",
        "#We unzip our images into another folder.\n",
        "import zipfile\n",
        "zippath = 'img_align_celeba.zip'\n",
        "targetfolder = 'all_images'\n",
        "\n",
        "with zipfile.ZipFile(zippath, 'r') as zip_ref:\n",
        "    zip_ref.extractall(targetfolder)\n",
        "\n",
        "# the image filenames need to be in their own column called \"image_names\"\n",
        "attributes = attributes.reset_index()\n",
        "attributes.rename(columns={'index': 'image_names'}, inplace=True)\n",
        "\n",
        "#The data is already partitioned into train, validation, and test datasets, so we read\n",
        "#these into separate dataframes in preparation for using ImageDataGenerators to\n",
        "#load each of our images for training.\n",
        "train = attributes[attributes['dataset'] == 0]\n",
        "train.drop(columns='dataset', inplace=True)\n",
        "train = train.iloc[:1000]\n",
        "valid = attributes[attributes['dataset'] == 1]\n",
        "valid.drop(columns='dataset', inplace=True)\n",
        "valid = valid.iloc[:1000]\n",
        "test = attributes[attributes['dataset'] == 2]\n",
        "test.drop(columns='dataset', inplace=True)\n",
        "test = test.iloc[:1000]\n",
        "\n",
        "batch_size = 64\n",
        "\n",
        "attribute_names = ['5_o_Clock_Shadow', 'Arched_Eyebrows', 'Attractive', 'Bags_Under_Eyes', 'Bald', 'Bangs', 'Big_Lips', 'Big_Nose', 'Black_Hair', 'Blond_Hair',\n",
        "'Blurry', 'Brown_Hair', 'Bushy_Eyebrows', 'Chubby', 'Double_Chin', 'Eyeglasses', 'Goatee', 'Gray_Hair', 'Heavy_Makeup', 'High_Cheekbones', 'Male', 'Mouth_Slightly_Open',\n",
        "                    'Mustache', 'Narrow_Eyes', 'No_Beard', 'Oval_Face', 'Pale_Skin', 'Pointy_Nose', 'Receding_Hairline', 'Rosy_Cheeks', 'Sideburns', 'Smiling',\n",
        "                    'Straight_Hair', 'Wavy_Hair', 'Wearing_Earrings', 'Wearing_Hat', 'Wearing_Lipstick', 'Wearing_Necklace', 'Wearing_Necktie', 'Young']\n",
        "\n",
        "# replace all -1s with 0s for binary classification\n",
        "for name in attribute_names:\n",
        "  train[name].replace({-1: 0}, inplace=True)\n",
        "  valid[name].replace({-1: 0}, inplace=True)\n",
        "  test[name].replace({-1: 0}, inplace=True)\n",
        "\n",
        "# Use ImageDataGenerator for train and valid datasets\n",
        "train_datagen = ImageDataGenerator(rescale=1.0/255)\n",
        "valid_datagen = ImageDataGenerator(rescale=1.0/255)\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# Flow from dataframe for train and valid generators\n",
        "train_generator = train_datagen.flow_from_dataframe(\n",
        "    dataframe=train,\n",
        "    directory='all_images/img_align_celeba/',\n",
        "    x_col=\"image_names\",\n",
        "    y_col=attribute_names,\n",
        "    class_mode=\"raw\",\n",
        "    batch_size=batch_size,\n",
        "    target_size=(109,89)\n",
        ")\n",
        "\n",
        "valid_generator = valid_datagen.flow_from_dataframe(\n",
        "    dataframe=valid,\n",
        "    directory='all_images/img_align_celeba/',\n",
        "    x_col=\"image_names\",\n",
        "    y_col=attribute_names,\n",
        "    class_mode=\"raw\",\n",
        "    batch_size=batch_size,\n",
        "    target_size=(109,89)\n",
        ")\n",
        "\n",
        "test_generator = test_datagen.flow_from_dataframe(\n",
        "    dataframe=test,\n",
        "    directory='all_images/img_align_celeba/',\n",
        "    x_col=\"image_names\",\n",
        "    y_col=attribute_names,\n",
        "    class_mode=\"raw\",\n",
        "    batch_size=batch_size,\n",
        "    target_size=(109,89)\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Initial Sequential Model"
      ],
      "metadata": {
        "id": "bqKRK8KewpZP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Conv2D(75, (3, 3), strides=1, padding=\"same\", activation=\"relu\",\n",
        "                 input_shape=(109, 89, 3)))\n",
        "model.add(Conv2D(50, (3, 3), strides=1, padding=\"same\", activation=\"relu\"))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(units=32, activation=\"relu\"))\n",
        "model.add(Dense(units=40, activation=\"sigmoid\"))\n",
        "model.add(Dropout(0.1))\n",
        "model.summary()\n",
        "\n",
        "# Compile and train your model\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "early_stopping = EarlyStopping(\n",
        "    patience=5,\n",
        "    min_delta=0.001,\n",
        "    monitor = 'accuracy',\n",
        "    restore_best_weights=True\n",
        ")\n",
        "\n",
        "model.fit_generator(generator=train_generator,\n",
        "                    epochs=1000,\n",
        "                    validation_data=valid_generator,\n",
        "                    verbose=1,\n",
        "                    callbacks = [early_stopping])"
      ],
      "metadata": {
        "id": "YomQdMJtwoWT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#Pre-Trained model"
      ],
      "metadata": {
        "id": "9zpJA8z6wsm-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#We load the pretrained ImageNet ResNet50 model, remove the top layers,\n",
        "#add our own pooling and dense layers, as well as a Dropout to prevent overfitting,\n",
        "#freeze the base layers, and train the model.\n",
        "\n",
        "input_shape = (109, 89, 3)\n",
        "input_layer = Input(shape=input_shape)\n",
        "resized_input = Resizing(224, 224)(input_layer) # resize images to 224x224 to fit ImageNet\n",
        "\n",
        "# Load the pre-trained ResNet50 model (excluding the top classification layers)\n",
        "base_model = ResNet50(weights='imagenet', include_top=False, input_tensor=resized_input)\n",
        "\n",
        "# Add custom layers on top of ResNet50\n",
        "x = base_model(base_model.input, training=False)\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dense(64, activation='relu', kernel_initializer=initializers.glorot_uniform())(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Dropout(0.3)(x)\n",
        "predictions = Dense(40, activation='sigmoid')(x)\n",
        "\n",
        "model = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "# Freeze the base ResNet50 layetrainrs\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Compile and train the model\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy', 'binary_accuracy'])\n",
        "\n",
        "# Define early stopping after 5 epochs\n",
        "early_stopping = EarlyStopping(\n",
        "    patience=5,\n",
        "    min_delta=0.001,\n",
        "    monitor = 'accuracy',\n",
        "    restore_best_weights=True\n",
        ")\n",
        "\n",
        "# Train on the training data for 10 epochs\n",
        "model.fit(train_generator,\n",
        "                    epochs=10,\n",
        "                    validation_data=valid_generator,\n",
        "                   steps_per_epoch=train.shape[0]//batch_size,\n",
        "                    validation_steps=valid.shape[0]//batch_size,\n",
        "                    verbose=1,\n",
        "                    callbacks = [early_stopping])\n",
        "\n",
        "#We fine-tune the model by unfreezing the top 25 layers and training with a smaller learning rate.\n",
        "for layer in model.layers[25:]:\n",
        "    layer.trainable = True\n",
        "\n",
        "# Recompile the model after fine-tuning\n",
        "model.compile(optimizer=Adam(learning_rate=0.0001), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Continue training with fine-tuning\n",
        "model.fit(train_generator, validation_data=valid_generator, steps_per_epoch=train.shape[0]//batch_size, epochs=10, callbacks = [early_stopping])"
      ],
      "metadata": {
        "id": "AqCxifZpwm79"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Evaluation"
      ],
      "metadata": {
        "id": "4PLCDgrBw9kY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_accuracy = model.evaluate(test_generator, verbose=1)\n",
        "print(\"Test Loss:\", test_loss)\n",
        "print(\"Test Accuracy:\", test_accuracy)"
      ],
      "metadata": {
        "id": "Up9ueRM1w6Gw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Classification Report"
      ],
      "metadata": {
        "id": "8sVzsCRLzHaf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for name in attribute_names:\n",
        "  test[name].replace({-1: 0}, inplace=True)\n",
        "\n",
        "test_generator = test_datagen.flow_from_dataframe(\n",
        "    dataframe=test,\n",
        "    directory='all_images/img_align_celeba/',\n",
        "    x_col=\"image_names\",\n",
        "    y_col=attribute_names,\n",
        "    class_mode=\"raw\",\n",
        "    batch_size=batch_size,\n",
        "    target_size=(109,89)\n",
        ")\n",
        "\n",
        "predictions = model.predict(test_generator)\n",
        "predictions_rounded = np.round(predictions)\n",
        "true_labels = test_generator.labels\n",
        "true_labels_binary = (true_labels == 1).astype(int)\n",
        "true_labels_binary_reshaped = true_labels_binary.reshape(predictions_rounded.shape)\n",
        "\n",
        "\n",
        "print(classification_report(true_labels_binary_reshaped, predictions_rounded, target_names=attribute_names))"
      ],
      "metadata": {
        "id": "DgOndbxuxH3O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Confusion Matrix (text)"
      ],
      "metadata": {
        "id": "-hDSSnxjzI1X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import multilabel_confusion_matrix\n",
        "import numpy as np\n",
        "\n",
        "# Define an empty array to store the predictions and true labels for all batches from the test generator\n",
        "all_predictions = []\n",
        "all_true_labels = []\n",
        "\n",
        "# Loop through the test generator to obtain predictions and true labels for all batches\n",
        "test_generator.reset()  # Reset the test generator to starttrue_labels = test_generator.labels from the beginning\n",
        "for i in range(len(test_generator)):       # run for however many batches we want, I started with two because it took a while\n",
        "    batch_data, batch_labels = test_generator.next()\n",
        "    batch_predictions = model.predict(batch_data)\n",
        "    all_predictions.append(batch_predictions)\n",
        "    all_true_labels.append(batch_labels)\n",
        "\n",
        "\n",
        "# Concatenate the predictions and true labels from all batches into single arrays\n",
        "predictions = np.concatenate(all_predictions)\n",
        "true_labels = np.concatenate(all_true_labels)\n",
        "\n",
        "# Convert to binary\n",
        "true_labels_binary = (true_labels == 1).astype(int)\n",
        "predictions_binary = (predictions > 0.5).astype(int)\n",
        "\n",
        "# Generate the multilabel confusion matrix\n",
        "confusion_matrix = multilabel_confusion_matrix(true_labels_binary, predictions_binary)  # threshold\n",
        "\n",
        "# Reshape and print the multilabel confusion matrix\n",
        "confusion_matrix = confusion_matrix.reshape(40, 2, 2)\n",
        "print(confusion_matrix)"
      ],
      "metadata": {
        "id": "EVN3qnGwxJ-o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Confusion Matrix (plots)"
      ],
      "metadata": {
        "id": "l9rk1KaTzLst"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axes = plt.subplots(10, 4, figsize=(20, 40))  # Create a 10x4 grid of subplots\n",
        "for i in range(40):\n",
        "    ax = axes[i // 4, i % 4]  # Get the current subplot\n",
        "    sns.heatmap(confusion_matrix[i], annot=True, cmap='Blues', fmt='d', cbar=False, ax=ax)\n",
        "    ax.set_xlabel('Predicted Label')\n",
        "    ax.set_ylabel('True Label')\n",
        "    ax.set_title(attribute_names[i])\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "IM3p6YqgxLsB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Looking at the images in the data"
      ],
      "metadata": {
        "id": "Oy27-wIDzP-n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_single_image(model, img_path, target_size):\n",
        "    # Load and preprocess the image\n",
        "    img = image.load_img(img_path, target_size=target_size)\n",
        "    plt.imshow(img)\n",
        "    plt.show()\n",
        "    img_array = image.img_to_array(img)\n",
        "    img_array = np.expand_dims(img_array, axis=0)  # Expand dimensions to create a batch with a single sample\n",
        "    img_array /= 255.0  # Normalize pixel values\n",
        "\n",
        "    # Make predictions\n",
        "    predictions = model.predict(img_array)\n",
        "    return predictions\n",
        "\n",
        "img_path = '/content/all_images/img_align_celeba/000008.jpg'\n",
        "target_size = (109, 89)  # Specify the target size used during training\n",
        "\n",
        "predictions = predict_single_image(model, img_path, target_size)\n",
        "# Print predictions in a more readable way\n",
        "for i, prob in enumerate(predictions[0]):\n",
        "    print(f\"Probability of {attribute_names[i]}: {prob:.4f}\")"
      ],
      "metadata": {
        "id": "lI7COZxDxNWy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Looking at our own images"
      ],
      "metadata": {
        "id": "MusHO0NrzR8K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_real_picture(img_path):\n",
        "    img = image.load_img(img_path)\n",
        "    plt.imshow(img)\n",
        "    plt.show()\n",
        "    img_array = image.img_to_array(img)\n",
        "    img_array = np.expand_dims(img_array, axis=0)  # Expand dimensions to create a batch with a single sample\n",
        "    img_array /= 255.0  # Normalize pixel values\n",
        "\n",
        "    # Make predictions\n",
        "    predictions = model.predict(img_array)\n",
        "    for i, prob in enumerate(predictions[0]):\n",
        "      print(f\"Probability of {attribute_names[i]}: {prob:.4f}\")\n",
        "\n",
        "predict_real_picture(\"/content/drive/MyDrive/selfie.png\")\n",
        "predict_real_picture(\"/content/drive/MyDrive/sam.jpg\")"
      ],
      "metadata": {
        "id": "2_OzyHdExO5v"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}