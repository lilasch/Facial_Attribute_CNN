{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM2kmHLIVJpX6wJgnuqG6sc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lilasch/Facial_Attribute_CNN/blob/main/Final_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Data Collection and Training"
      ],
      "metadata": {
        "id": "JrHKHS6qxArV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KIDfpwbtwQ6B"
      },
      "outputs": [],
      "source": [
        "import tensorflow\n",
        "import tensorflow.keras as keras\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import sklearn\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.layers import Dense, Dropout, Input, GlobalAveragePooling2D, BatchNormalization\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers.experimental.preprocessing import Resizing\n",
        "from tensorflow.keras import initializers\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from keras.preprocessing import image\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import classification_report, multilabel_confusion_matrix\n",
        "\n",
        "\n",
        "# Load attributes csv\n",
        "attributes = pd.read_csv(\"list_attr_celeba.txt\", skiprows = 1, delimiter=\"\\s+|\\t\")\n",
        "\n",
        "# Load csv with partitions values\n",
        "partitions = pd.read_csv(\"list_eval_partition.txt\", delimiter=\"\\s+|\\t\", header = None)\n",
        "\n",
        "partitions.columns = ['image_name', 'dataset'] # setting column header names for partitions\n",
        "attributes['dataset'] = partitions['dataset'].values # copying the partition values into the attributes df\n",
        "\n",
        "# unzipping image data\n",
        "import zipfile\n",
        "zippath = 'img_align_celeba.zip'\n",
        "targetfolder = 'all_images'\n",
        "\n",
        "with zipfile.ZipFile(zippath, 'r') as zip_ref:\n",
        "    zip_ref.extractall(targetfolder)\n",
        "\n",
        "# the image filenames need to be in their own column called \"image_names\"\n",
        "attributes = attributes.reset_index()\n",
        "attributes.rename(columns={'index': 'image_names'}, inplace=True)\n",
        "\n",
        "# splitting the data\n",
        "train = attributes[attributes['dataset'] == 0]\n",
        "train.drop(columns='dataset', inplace=True)\n",
        "train = train.iloc[:1000]\n",
        "valid = attributes[attributes['dataset'] == 1]\n",
        "valid.drop(columns='dataset', inplace=True)\n",
        "valid = valid.iloc[:1000]\n",
        "test = attributes[attributes['dataset'] == 2]\n",
        "test.drop(columns='dataset', inplace=True)\n",
        "test = test.iloc[:1000]\n",
        "\n",
        "batch_size = 64\n",
        "\n",
        "attribute_names = ['5_o_Clock_Shadow', 'Arched_Eyebrows', 'Attractive', 'Bags_Under_Eyes', 'Bald', 'Bangs', 'Big_Lips', 'Big_Nose', 'Black_Hair', 'Blond_Hair',\n",
        "'Blurry', 'Brown_Hair', 'Bushy_Eyebrows', 'Chubby', 'Double_Chin', 'Eyeglasses', 'Goatee', 'Gray_Hair', 'Heavy_Makeup', 'High_Cheekbones', 'Male', 'Mouth_Slightly_Open',\n",
        "                    'Mustache', 'Narrow_Eyes', 'No_Beard', 'Oval_Face', 'Pale_Skin', 'Pointy_Nose', 'Receding_Hairline', 'Rosy_Cheeks', 'Sideburns', 'Smiling',\n",
        "                    'Straight_Hair', 'Wavy_Hair', 'Wearing_Earrings', 'Wearing_Hat', 'Wearing_Lipstick', 'Wearing_Necklace', 'Wearing_Necktie', 'Young']\n",
        "\n",
        "# replace all -1s with 0s for binary classification\n",
        "for name in attribute_names:\n",
        "  train[name].replace({-1: 0}, inplace=True)\n",
        "  valid[name].replace({-1: 0}, inplace=True)\n",
        "  test[name].replace({-1: 0}, inplace=True)\n",
        "\n",
        "# Use ImageDataGenerator for train and valid datasets\n",
        "train_datagen = ImageDataGenerator(rescale=1.0/255)\n",
        "valid_datagen = ImageDataGenerator(rescale=1.0/255)\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# Flow from dataframe for train and valid generators\n",
        "train_generator = train_datagen.flow_from_dataframe(\n",
        "    dataframe=train,\n",
        "    directory='all_images/img_align_celeba/',\n",
        "    x_col=\"image_names\",\n",
        "    y_col=attribute_names,\n",
        "    class_mode=\"raw\",\n",
        "    batch_size=batch_size,\n",
        "    target_size=(109,89)\n",
        ")\n",
        "\n",
        "valid_generator = valid_datagen.flow_from_dataframe(\n",
        "    dataframe=valid,\n",
        "    directory='all_images/img_align_celeba/',\n",
        "    x_col=\"image_names\",\n",
        "    y_col=attribute_names,\n",
        "    class_mode=\"raw\",\n",
        "    batch_size=batch_size,\n",
        "    target_size=(109,89)\n",
        ")\n",
        "\n",
        "test_generator = test_datagen.flow_from_dataframe(\n",
        "    dataframe=test,\n",
        "    directory='all_images/img_align_celeba/',\n",
        "    x_col=\"image_names\",\n",
        "    y_col=attribute_names,\n",
        "    class_mode=\"raw\",\n",
        "    batch_size=batch_size,\n",
        "    target_size=(109,89)\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Initial Sequential Model"
      ],
      "metadata": {
        "id": "bqKRK8KewpZP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Conv2D(75, (3, 3), strides=1, padding=\"same\", activation=\"relu\",\n",
        "                 input_shape=(109, 89, 3)))\n",
        "model.add(Conv2D(50, (3, 3), strides=1, padding=\"same\", activation=\"relu\"))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(units=32, activation=\"relu\"))\n",
        "model.add(Dense(units=40, activation=\"sigmoid\"))\n",
        "model.add(Dropout(0.1))\n",
        "model.summary()\n",
        "\n",
        "# Compile and train your model\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "early_stopping = EarlyStopping(\n",
        "    patience=5,\n",
        "    min_delta=0.001,\n",
        "    monitor = 'accuracy',\n",
        "    restore_best_weights=True\n",
        ")\n",
        "\n",
        "model.fit_generator(generator=train_generator,\n",
        "                    epochs=1000,\n",
        "                    validation_data=valid_generator,\n",
        "                    verbose=1,\n",
        "                    callbacks = [early_stopping])"
      ],
      "metadata": {
        "id": "YomQdMJtwoWT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#Pre-Trained model"
      ],
      "metadata": {
        "id": "9zpJA8z6wsm-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_shape = (109, 89, 3)\n",
        "input_layer = Input(shape=input_shape)\n",
        "resized_input = Resizing(224, 224)(input_layer) # resize images to 224x224 to fit ImageNet\n",
        "\n",
        "# Load the pre-trained ResNet50 model (excluding the top classification layers)\n",
        "base_model = ResNet50(weights='imagenet', include_top=False, input_tensor=resized_input)\n",
        "\n",
        "# Add custom layers on top of ResNet50\n",
        "x = base_model(base_model.input, training=False)\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dense(64, activation='relu', kernel_initializer=initializers.glorot_uniform())(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Dropout(0.3)(x)\n",
        "predictions = Dense(40, activation='sigmoid')(x)\n",
        "\n",
        "model = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "# Freeze the base ResNet50 layetrainrs\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Compile and train the model\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy', 'binary_accuracy'])\n",
        "\n",
        "# Define early stopping after 5 epochs\n",
        "early_stopping = EarlyStopping(\n",
        "    patience=5,\n",
        "    min_delta=0.001,\n",
        "    monitor = 'accuracy',\n",
        "    restore_best_weights=True\n",
        ")\n",
        "\n",
        "# Train on the training data for 10 epochs\n",
        "model.fit(train_generator,\n",
        "                    epochs=10,\n",
        "                    validation_data=valid_generator,\n",
        "                   steps_per_epoch=train.shape[0]//batch_size,\n",
        "                    validation_steps=valid.shape[0]//batch_size,\n",
        "                    verbose=1,\n",
        "                    callbacks = [early_stopping])\n",
        "\n",
        "# Fine-tune the model\n",
        "for layer in model.layers[25:]:\n",
        "    layer.trainable = True\n",
        "\n",
        "# Recompile the model after fine-tuning\n",
        "model.compile(optimizer=Adam(learning_rate=0.0001), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Continue training with fine-tuning\n",
        "model.fit(train_generator, validation_data=valid_generator, steps_per_epoch=train.shape[0]//batch_size, epochs=10, callbacks = [early_stopping])"
      ],
      "metadata": {
        "id": "AqCxifZpwm79"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Evaluation"
      ],
      "metadata": {
        "id": "4PLCDgrBw9kY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_accuracy = model.evaluate(test_generator, verbose=1)\n",
        "print(\"Test Loss:\", test_loss)\n",
        "print(\"Test Accuracy:\", test_accuracy)"
      ],
      "metadata": {
        "id": "Up9ueRM1w6Gw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for name in attribute_names:\n",
        "  test[name].replace({-1: 0}, inplace=True)\n",
        "\n",
        "test_generator = test_datagen.flow_from_dataframe(\n",
        "    dataframe=test,\n",
        "    directory='all_images/img_align_celeba/',\n",
        "    x_col=\"image_names\",\n",
        "    y_col=attribute_names,\n",
        "    class_mode=\"raw\",\n",
        "    batch_size=batch_size,\n",
        "    target_size=(109,89)\n",
        ")\n",
        "\n",
        "predictions = model.predict(test_generator)\n",
        "predictions_rounded = np.round(predictions)\n",
        "true_labels = test_generator.labels\n",
        "true_labels_binary = (true_labels == 1).astype(int)\n",
        "true_labels_binary_reshaped = true_labels_binary.reshape(predictions_rounded.shape)\n",
        "\n",
        "\n",
        "print(classification_report(true_labels_binary_reshaped, predictions_rounded, target_names=attribute_names))"
      ],
      "metadata": {
        "id": "DgOndbxuxH3O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import multilabel_confusion_matrix\n",
        "import numpy as np\n",
        "\n",
        "# Define an empty array to store the predictions and true labels for all batches from the test generator\n",
        "all_predictions = []\n",
        "all_true_labels = []\n",
        "\n",
        "# Loop through the test generator to obtain predictions and true labels for all batches\n",
        "test_generator.reset()  # Reset the test generator to starttrue_labels = test_generator.labels from the beginning\n",
        "for i in range(len(test_generator)):       # run for however many batches we want, I started with two because it took a while\n",
        "    batch_data, batch_labels = test_generator.next()\n",
        "    batch_predictions = model.predict(batch_data)\n",
        "    all_predictions.append(batch_predictions)\n",
        "    all_true_labels.append(batch_labels)\n",
        "\n",
        "\n",
        "# Concatenate the predictions and true labels from all batches into single arrays\n",
        "predictions = np.concatenate(all_predictions)\n",
        "true_labels = np.concatenate(all_true_labels)\n",
        "\n",
        "# Convert to binary\n",
        "true_labels_binary = (true_labels == 1).astype(int)\n",
        "predictions_binary = (predictions > 0.5).astype(int)\n",
        "\n",
        "# Generate the multilabel confusion matrix\n",
        "confusion_matrix = multilabel_confusion_matrix(true_labels_binary, predictions_binary)  # threshold\n",
        "\n",
        "# Reshape and print the multilabel confusion matrix\n",
        "confusion_matrix = confusion_matrix.reshape(40, 2, 2)\n",
        "print(confusion_matrix)"
      ],
      "metadata": {
        "id": "EVN3qnGwxJ-o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axes = plt.subplots(10, 4, figsize=(20, 40))  # Create a 10x4 grid of subplots\n",
        "for i in range(40):\n",
        "    ax = axes[i // 4, i % 4]  # Get the current subplot\n",
        "    sns.heatmap(confusion_matrix[i], annot=True, cmap='Blues', fmt='d', cbar=False, ax=ax)\n",
        "    ax.set_xlabel('Predicted Label')\n",
        "    ax.set_ylabel('True Label')\n",
        "    ax.set_title(attribute_names[i])\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "IM3p6YqgxLsB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_single_image(model, img_path, target_size):\n",
        "    # Load and preprocess the image\n",
        "    img = image.load_img(img_path, target_size=target_size)\n",
        "    plt.imshow(img)\n",
        "    plt.show()\n",
        "    img_array = image.img_to_array(img)\n",
        "    img_array = np.expand_dims(img_array, axis=0)  # Expand dimensions to create a batch with a single sample\n",
        "    img_array /= 255.0  # Normalize pixel values\n",
        "\n",
        "    # Make predictions\n",
        "    predictions = model.predict(img_array)\n",
        "    return predictions\n",
        "\n",
        "img_path = '/content/all_images/img_align_celeba/000008.jpg'\n",
        "target_size = (109, 89)  # Specify the target size used during training\n",
        "\n",
        "predictions = predict_single_image(model, img_path, target_size)\n",
        "# Print predictions in a more readable way\n",
        "for i, prob in enumerate(predictions[0]):\n",
        "    print(f\"Probability of {attribute_names[i]}: {prob:.4f}\")"
      ],
      "metadata": {
        "id": "lI7COZxDxNWy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_real_picture(img_path):\n",
        "    img = image.load_img(img_path)\n",
        "    plt.imshow(img)\n",
        "    plt.show()\n",
        "    img_array = image.img_to_array(img)\n",
        "    img_array = np.expand_dims(img_array, axis=0)  # Expand dimensions to create a batch with a single sample\n",
        "    img_array /= 255.0  # Normalize pixel values\n",
        "\n",
        "    # Make predictions\n",
        "    predictions = model.predict(img_array)\n",
        "    for i, prob in enumerate(predictions[0]):\n",
        "      print(f\"Probability of {attribute_names[i]}: {prob:.4f}\")\n",
        "\n",
        "predict_real_picture(\"/content/drive/MyDrive/selfie.png\")\n",
        "predict_real_picture(\"/content/drive/MyDrive/sam.jpg\")"
      ],
      "metadata": {
        "id": "2_OzyHdExO5v"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}